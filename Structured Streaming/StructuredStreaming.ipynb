{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4f444c0-4134-449e-9299-3949f4105f3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run /Workspace/Users/aminah.yussuf@slalom.com/Includes/Copy-Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7f1f6b0-3f42-4c8d-8db2-1c6c51dab21e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS books_csv\n",
    "  (book_id STRING, title STRING, author STRING, category STRING, price DOUBLE)\n",
    "USING CSV\n",
    "OPTIONS ( \n",
    "  header = \"true\", \n",
    "  delimiter = \";\"\n",
    ")\n",
    "LOCATION \"dbfs:/mnt/demo-datasets/bookstore/books-csv\";\n",
    "\n",
    "\n",
    "select * from books_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5505b74-811b-4159-bacd-c12920c8cfda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(spark.readStream\n",
    " .table(\"books_csv\")\n",
    " .createOrReplaceTempView(\"books_streaming_tmp_vw\")\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb6d9fdb-4f6e-4f8a-a842-ee9d9f3e0ee0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from books_streaming_tmp_vw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0101228-8c91-4b62-9ee9-f8d46ab1a329",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT author, count(book_id) AS total_books \n",
    "FROM books_streaming_tmp_vw\n",
    "GROUP BY author"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11fea109-f5a5-4174-9254-22f0bcdb9be6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "----create a new streaming temporary view \n",
    "CREATE OR REPLACE TEMP VIEW author_counts_tmp_vw AS (\n",
    "  SELECT author, count(book_id) AS total_books\n",
    "  FROM books_streaming_tmp_vw\n",
    "  GROUP BY author\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43bf5c4e-ffb9-4bb5-85ad-d017ee0138dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#use spark table to load a streaming temporary view into a dataframe. Spark always load streaming views as a streaming dataframe and static views as static dataframe. Here we use dataframe wrriteStream method to persist the result of a streaming query to a durable storage\n",
    "(spark.table(\"author_counts_tmp_vw\")\n",
    "    .writeStream\n",
    "    .trigger(processingTime='4 seconds')\\\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/demo-datasets/author_counts_tmp_vw_checkpoint\")\n",
    "    .table(\"author_counts\")\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6658bae6-dabc-42ec-97b4-d6ff26bbdddd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from author_counts;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ceb0d183-afac-4a4f-876c-612e965a3a86",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#a triggered incremental batch, in this case the query will process all new available data and stop when it is done. we use awaitTermination=true method to block the execution of any cell in this notebook until the incremental batch write succeeds. \n",
    "(spark.table(\"author_counts_tmp_vw\")\n",
    "    .writeStream\n",
    "    .trigger(availableNow=True)\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"checkpointLocation\", \"dbfs:/mnt/demo-datasets/author_counts_tmp_vw_checkpoint\")\n",
    "    .table(\"author_counts\")\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "36bba754-9651-4455-9c43-ae7fa56f6eae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from author_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0cd1da8-40c5-4f52-99aa-94c489aca0f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6970358119262390,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "StructuredStreaming",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
